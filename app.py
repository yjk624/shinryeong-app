import streamlit as st
from groq import Groq
from saju_engine import calculate_saju_v3
from datetime import datetime, time
from geopy.geocoders import Nominatim
from geopy.exc import GeocoderTimedOut, GeocoderServiceError
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import os

# ==========================================
# 1. CONFIGURATION
# ==========================================
st.set_page_config(page_title="ì‹ ë ¹ (Shinryeong)", page_icon="ğŸ”®", layout="centered")

# Robust Geocoding
geolocator = Nominatim(user_agent="shinryeong_app_v11_auto_switch", timeout=10)

# Initialize Groq
try:
    GROQ_KEY = st.secrets["GROQ_API_KEY"]
    client = Groq(api_key=GROQ_KEY)
except Exception as e:
    st.error(f"ğŸš¨ Connection Error: {e}")
    st.stop()

# Session State
if "messages" not in st.session_state: st.session_state.messages = []
if "saju_context" not in st.session_state: st.session_state.saju_context = ""
if "user_info_logged" not in st.session_state: st.session_state.user_info_logged = False

# ==========================================
# 2. FILE LOADERS
# ==========================================
@st.cache_data
def load_text_file(filename):
    try:
        with open(filename, "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return ""

PROMPT_TEXT = load_text_file("prompt.txt")
KNOWLEDGE_TEXT = load_text_file("knowledgebase.txt")

# ==========================================
# 3. HELPER FUNCTIONS & SMART AI ENGINE
# ==========================================
CITY_DB = {
    "ì„œìš¸": (37.56, 126.97), "Seoul": (37.56, 126.97),
    "ë¶€ì‚°": (35.17, 129.07), "Busan": (35.17, 129.07),
    "ì¸ì²œ": (37.45, 126.70), "Incheon": (37.45, 126.70),
    "ëŒ€êµ¬": (35.87, 128.60), "Daegu": (35.87, 128.60),
    "ëŒ€ì „": (36.35, 127.38), "Daejeon": (36.35, 127.38),
    "ê´‘ì£¼": (35.15, 126.85), "Gwangju": (35.15, 126.85),
    "ìš¸ì‚°": (35.53, 129.31), "Ulsan": (35.53, 129.31),
    "ì„¸ì¢…": (36.48, 127.28), "Sejong": (36.48, 127.28),
    "ì°½ì›": (35.22, 128.68), "Changwon": (35.22, 128.68),
    "ìˆ˜ì›": (37.26, 127.02), "Suwon": (37.26, 127.02),
    "ì œì£¼": (33.49, 126.53), "Jeju": (33.49, 126.53),
    "ê°•ë¦‰": (37.75, 128.87), "Gangneung": (37.75, 128.87),
    "New York": (40.71, -74.00), "London": (51.50, -0.12),
    "Paris": (48.85, 2.35), "Tokyo": (35.67, 139.65)
}

def get_coordinates(city_input):
    clean = city_input.strip()
    if clean in CITY_DB: return CITY_DB[clean], clean
    for city_key, coords in CITY_DB.items():
        if city_key in clean or city_key.lower() in clean.lower():
            return coords, city_key 
    try:
        loc = geolocator.geocode(clean)
        if loc: return (loc.latitude, loc.longitude), clean
    except: pass
    return None, None

def save_to_database(user_data, birth_date_obj, birth_time_obj, concern, is_lunar):
    try:
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        creds_dict = dict(st.secrets["gcp_service_account"])
        creds_dict["private_key"] = creds_dict["private_key"].replace("\\n", "\n")
        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
        gs_client = gspread.authorize(creds)
        sheet = gs_client.open("Shinryeong_User_Data").sheet1
        cal_type = "Lunar" if is_lunar else "Solar"
        row = [
            datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            f"{birth_date_obj.strftime('%Y-%m-%d')} ({cal_type})",
            birth_time_obj.strftime("%H:%M"),
            str(user_data.get('Birth_Place', 'Unknown')),
            user_data.get('Gender', 'Unknown'),
            user_data.get('Year', ''),
            user_data.get('Month', ''),
            user_data.get('Day', ''),
            user_data.get('Time', ''),
            concern
        ]
        sheet.append_row(row)
    except: pass

# [SMART ENGINE SWITCHER]
def generate_ai_response(messages):
    # Priority List: Best Quality -> Fast/Backup -> Large Context Backup
    models_to_try = [
        "llama-3.3-70b-versatile",  # Best Quality (Hit Limit)
        "llama-3.1-8b-instant",     # Fast Backup (Separate Quota)
        "mixtral-8x7b-32768"        # Safety Net
    ]
    
    for model_name in models_to_try:
        try:
            stream = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.6,
                max_tokens=3000,
                top_p=1,
                stream=True,
                stop=None,
            )
            
            # If successful, yield the stream
            full_response = ""
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    yield content
            
            # If we finish the loop without error, stop trying models
            return
            
        except Exception as e:
            error_msg = str(e)
            # If Rate Limit (429), try next model. If other error, keep trying.
            if "429" in error_msg or "rate limit" in error_msg.lower():
                print(f"âš ï¸ Model {model_name} exhausted. Switching...")
                continue # Try next model in list
            else:
                yield f"Error with {model_name}: {error_msg}"
                return

# ==========================================
# 4. UI LAYOUT
# ==========================================
TRANS = {
    "ko": {
        "title": "ğŸ”® ì‹ ë ¹ (Shinryeong)",
        "subtitle": "AI í˜•ì´ìƒí•™ ë¶„ì„ê°€",
        "warning": """
        âš–ï¸ **ë²•ì  ë©´ì±… ì¡°í•­ (Disclaimer):**
        1. ë³¸ ì„œë¹„ìŠ¤ëŠ” ëª…ë¦¬í•™ ë° ìë¯¸ë‘ìˆ˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ **í•™ìˆ ì  ë¶„ì„**ì´ë©°, ì ˆëŒ€ì ì¸ ì˜ˆì–¸ì´ ì•„ë‹™ë‹ˆë‹¤.
        2. ì‹ ë ¹ì€ **ì˜í•™ì  ì§„ë‹¨(Medical Diagnosis)ì´ë‚˜ ë²•ë¥ ì  ì¡°ì–¸(Legal Advice)**ì„ ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        3. ë³¸ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì‚¬ìš©ìì˜ ê²°ì •ê³¼ ê·¸ ê²°ê³¼ì— ëŒ€í•œ ì±…ì„ì€ ì „ì ìœ¼ë¡œ **ì‚¬ìš©ì ë³¸ì¸**ì—ê²Œ ìˆìŠµë‹ˆë‹¤.
        """,
        "submit_btn": "ğŸ”® ì‹ ë ¹ì—ê²Œ ë¶„ì„ ìš”ì²­í•˜ê¸°",
        "loading": "â³ ì²œë¬¸ ë°ì´í„°ë¥¼ ê³„ì‚°í•˜ê³  ì‹ ë ¹ì„ ì†Œí™˜í•˜ëŠ” ì¤‘...",
        "geo_error": "âš ï¸ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì£¼ìš” ë„ì‹œëª…ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
        "chat_placeholder": "ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”? (ì˜ˆ: ë‚´ë…„ì˜ ì¬ë¬¼ìš´ì€?)",
        "reset_btn": "ğŸ”„ ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘",
        "dob_label": "ìƒë…„ì›”ì¼", "time_label": "íƒœì–´ë‚œ ì‹œê°„", "gender_label": "ì„±ë³„",
        "male": "ë‚¨ì„±", "female": "ì—¬ì„±", "loc_label": "íƒœì–´ë‚œ ì§€ì—­ (ë„ì‹œëª…)",
        "concern_label": "í˜„ì¬ ê°€ì¥ í° ê³ ë¯¼ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "cal_label": "ì–‘ë ¥/ìŒë ¥ êµ¬ë¶„",
        "theory_header": "ğŸ“š ë¶„ì„ ê·¼ê±° (Technical Basis)"
    },
    "en": {
        "title": "ğŸ”® Shinryeong",
        "subtitle": "AI Metaphysical Analyst",
        "warning": """
        âš–ï¸ **Legal Disclaimer:**
        1. This service provides **academic analysis** based on Saju and Jami Dou Shu data; it is not absolute prophecy.
        2. Shinryeong does **NOT provide Medical Diagnoses or Legal Advice**.
        3. The user bears full responsibility for any decisions made based on this analysis.
        """,
        "submit_btn": "ğŸ”® Request Analysis",
        "loading": "â³ Calculating celestial data...",
        "geo_error": "âš ï¸ Location not found.",
        "chat_placeholder": "Follow-up questions?",
        "reset_btn": "ğŸ”„ Start New Analysis",
        "dob_label": "Date of Birth", "time_label": "Time of Birth", "gender_label": "Gender",
        "male": "Male", "female": "Female", "loc_label": "Birth Place (City)",
        "concern_label": "What is your main concern?",
        "cal_label": "Calendar Type",
        "theory_header": "ğŸ“š Technical Basis"
    }
}

with st.sidebar:
    lang_code = "ko" if st.radio("Language / ì–¸ì–´", ["í•œêµ­ì–´", "English"]) == "í•œêµ­ì–´" else "en"
    txt = TRANS[lang_code]
    if st.button(txt["reset_btn"]):
        st.session_state.messages = []
        st.session_state.saju_context = {}
        st.session_state.user_info_logged = False
        st.rerun()
    st.caption("Engine: Groq Auto-Switch")

st.title(txt["title"])
st.caption(txt["subtitle"])
st.info(txt["warning"])

# ==========================================
# 5. MAIN LOGIC
# ==========================================
if not st.session_state.saju_context:
    with st.form("input"):
        col1, col2 = st.columns(2)
        with col1:
            b_date = st.date_input(txt["dob_label"], min_value=datetime(1940,1,1))
            b_time = st.time_input(txt["time_label"], value=time(12,00), step=60)
            cal_type = st.radio(txt["cal_label"], ["ì–‘ë ¥ (Solar)", "ìŒë ¥ (Lunar)"])
        with col2:
            gender = st.radio(txt["gender_label"], [txt["male"], txt["female"]])
            loc_in = st.text_input(txt["loc_label"], placeholder="Seoul, Busan...")
        q = st.text_area(txt["concern_label"], height=100)
        submitted = st.form_submit_button(txt["submit_btn"])

    if submitted:
        if not loc_in:
            st.error(txt["geo_error"])
        else:
            with st.spinner(txt["loading"]):
                coords, matched_city = get_coordinates(loc_in)
                
                if coords:
                    lat, lon = coords
                    is_lunar = True if "ìŒë ¥" in cal_type else False
                    city_name = matched_city if matched_city else loc_in
                    
                    saju = calculate_saju_v3(b_date.year, b_date.month, b_date.day, 
                                           b_time.hour, b_time.minute, lat, lon, is_lunar)
                    saju['Birth_Place'] = city_name
                    saju['Gender'] = gender
                    
                    # CSV Format Display
                    csv_display = f"""
                    | Parameter | Value |
                    | :--- | :--- |
                    | **Date** | {b_date} ({cal_type}) |
                    | **Time** | {b_time} |
                    | **Location** | {city_name} |
                    | **Gender** | {gender} |
                    | **Saju** | {saju['Year']} / {saju['Month']} / {saju['Day']} / {saju['Time']} |
                    """
                    
                    system_prompt = f"""
                    [SYSTEM ROLE]
                    You are 'Shinryeong' (ì‹ ë ¹). Speak in 'Hage-che' (í•˜ê²Œì²´).
                    Language: {lang_code.upper()} Only.
                    
                    [KNOWLEDGE BASE]
                    {KNOWLEDGE_TEXT}
                    
                    [USER DATA]
                    - Saju: {saju}
                    - Gender: {gender}
                    - Concern: "{q}"
                    
                    [OUTPUT TEMPLATE]
                    ### ğŸ“œ ì‹ ë ¹ì˜ ë¶„ì„ ë³´ê³ ì„œ
                    
                    {csv_display}
                    
                    ---
                    
                    ### [Icon] 1. íƒ€ê³ ë‚œ ê¸°ì§ˆê³¼ ì—ë„ˆì§€
                    (Analyze the 4 Pillars. Use metaphors.)
                    
                    ### [Icon] 2. ğŸ” ì‹ ë ¹ì˜ ê³µëª… (Accuracy Check)
                    (Cold reading deduction.)
                    
                    ### [Icon] 3. âš¡ í˜„ì¬ì˜ íë¦„ê³¼ ë¦¬ìŠ¤í¬
                    (Address concern.)
                    
                    ### [Icon] 4. ğŸ›¡ï¸ ì‹ ë ¹ì˜ ì²˜ë°© (Action Plan)
                    * **[Icon] í–‰ë™ ì§€ì¹¨:** ...
                    * **[Icon] ë§ˆìŒê°€ì§:** ...
                    
                    [[TECHNICAL_SECTION]]
                    (Technical theory.)
                    """
                    
                    st.session_state.saju_context = system_prompt
                    
                    msgs = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": f"Analyze my Saju. My concern is: {q}"}
                    ]
                    
                    response_container = st.empty()
                    full_text = ""
                    
                    # Stream and capture full text
                    for chunk in generate_ai_response(msgs):
                        full_text += chunk
                        response_container.markdown(full_text + "â–Œ")
                    
                    # Final Render with Split
                    response_container.empty()
                    if "[[TECHNICAL_SECTION]]" in full_text:
                        parts = full_text.split("[[TECHNICAL_SECTION]]")
                        main_report = parts[0]
                        theory_report = parts[1]
                    else:
                        main_report = full_text
                        theory_report = "Technical basis integrated."

                    st.markdown(main_report)
                    with st.expander(txt["theory_header"]):
                        st.markdown(theory_report)

                    st.session_state.messages.append({"role": "user", "content": q})
                    st.session_state.messages.append({"role": "assistant", "content": main_report, "theory": theory_report})
                    
                    if not st.session_state.user_info_logged:
                        save_to_database(saju, b_date, b_time, q, is_lunar)
                        st.session_state.user_info_logged = True
                else:
                    st.error(txt["geo_error"])
else:
    st.markdown("---")
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])
            if "theory" in m:
                with st.expander(txt["theory_header"]):
                    st.markdown(m["theory"])
            
    if p := st.chat_input(txt["chat_placeholder"]):
        st.session_state.messages.append({"role": "user", "content": p})
        with st.chat_message("user"): st.markdown(p)
        
        # Optimize Context for Follow-ups (Only send last 2 turns + Prompt)
        msgs = [{"role": "system", "content": st.session_state.saju_context}]
        for m in st.session_state.messages[-4:]: # Keep only last 4 messages to save tokens
            msgs.append({"role": m["role"], "content": m["content"]})
            
        with st.chat_message("assistant"):
            response_container = st.empty()
            full_text = ""
            for chunk in generate_ai_response(msgs):
                full_text += chunk
                response_container.markdown(full_text + "â–Œ")
            
            response_container.empty()
            if "[[TECHNICAL_SECTION]]" in full_text:
                parts = full_text.split("[[TECHNICAL_SECTION]]")
                main_r, theory_r = parts[0], parts[1]
            else:
                main_r, theory_r = full_text, ""
                
            st.markdown(main_r)
            if theory_r:
                with st.expander(txt["theory_header"]):
                    st.markdown(theory_r)
                    
            st.session_state.messages.append({"role": "assistant", "content": main_r, "theory": theory_r})
